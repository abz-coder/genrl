log_dir: /home/gensyn/shared/test

training:
  max_round: 1
  max_stage: 3
  prune_K: 5

game_manager:
  _target_: genrl_swarm.game.BaseGameManager
  max_stage: ${training.max_stage}
  max_round: ${training.max_round}
  prune_K: ${training.prune_K}
  game_state: 
    _target_: genrl_swarm.state.GameState
    round: 0
    stage: 0
  reward_manager:
    _target_: genrl_swarm.rewards.DefaultRewardManager
    reward_fn_store:
      _target_: genrl_swarm.rewards.reward_store.RewardFnStore
      max_rounds: ${training.max_round}
      reward_fn_stores:
        - _target_: genrl_swarm.rewards.reward_store.RoundRewardFnStore
          num_stages: ${training.max_stage}
          reward_fns:
            - _target_: genrl_swarm.examples.gsm8k.rewards.Stage0Rewards
            - _target_: genrl_swarm.examples.gsm8k.rewards.Stage1Rewards
            - _target_: genrl_swarm.examples.gsm8k.rewards.Stage2Rewards
  trainer:
    _target_: genrl_swarm.examples.gsm8k.trainer.GRPOTrainerModule
    models:
      - _target_: transformers.AutoModelForCausalLM.from_pretrained
        pretrained_model_name_or_path: /home/gensyn/shared/models/qwen-2.5-0.5B-Instruct
        config: trl.trainer.GRPOConfig
    log_with: tensorboard
  data_manager:
    _target_: genrl_swarm.examples.gsm8k.data.GSM8KDataManager
    # num_students_to_sample, num_critics_to_sample, subsampling_method, prompt_generator_role use defaults
  communication:
    _target_: genrl_swarm.communication.hivemind.hivemind_backend.HivemindBackend
  run_mode: "train"